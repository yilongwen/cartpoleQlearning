{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement learning\n",
    "\n",
    "In this notebook, we will apply reinforcement learning to train a model to play the 'CartPole' game. This is a classic control game, in which the player will be rewarded by balancing a pole on a cart. The longer it goes on, the bigger the reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Gym is a toolkit for developing and comparing reinforcement learning algorithms \n",
    "# !pip install gym \n",
    "import gym\n",
    "\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Batch_size is the number of states our agent replays, in order to learn\n",
    "batch_size = 32\n",
    "\n",
    "# We want the agent to play the game 50 times\n",
    "n_episodes = 51\n",
    "\n",
    "# Make a folder to store output, if it is not there already\n",
    "output_dir = \"model_output/CartPole\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define agent\n",
    "\n",
    "Our agent is the one who takes actions in the environment. In the CartPole game, the actions are 0 or 1 (push cart to left or right). The environment is defined by 4 state variables: \n",
    "\n",
    "0: Cart Position, (-4.8, 4.8)\n",
    "\n",
    "1: Cart Velocity, (-Inf, Inf)\n",
    "\n",
    "2: Pole Angle, (-24 deg, 24 deg)\n",
    "\n",
    "3: Pole Velocity At Tip, (-Inf, Inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size # state_size is the amount of variables which define our state\n",
    "        self.action_size = action_size # action_size is the amount of actions the agent can take\n",
    "        self.memory = deque(maxlen=2000) # the maximum amount of 'things' the agent remembers from the past\n",
    "        self.gamma = 0.99 # discount factor for the future rewards\n",
    "        self.epsilon = 1.0 # rate at which agent randomly takes actions\n",
    "        self.epsilon_decay = 0.995 \n",
    "        self.epsilon_min = 0.01\n",
    "        self.learning_rate = 0.001 \n",
    "        \n",
    "        self.model = self._build_model()\n",
    "        \n",
    "    # This is the 'brain' of our agent, it predicts which action to take based on the state values    \n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        act_values = self.model.predict(state)\n",
    "        \n",
    "        return np.argmax(act_values)\n",
    "      \n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, done in mini_batch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                # The famous Bellman equation\n",
    "                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            \n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "        \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(state_size, action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with the environment\n",
    "\n",
    "1. Agent takes random action(initially) and steps through the environment. This yields some outcome and reward\n",
    "2. Agent remembers what happened.\n",
    "3. If agent has enough memory, it replays and tries to 'learn' to do better.\n",
    "4. Repeat 1-3 until the episode is over.\n",
    "5. Repeat 1-4 for n_episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/51, score: 25, epsilon: 1.0\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Episode 1/51, score: 11, epsilon: 0.9752487531218751\n",
      "Episode 2/51, score: 15, epsilon: 0.9046104802746175\n",
      "Episode 3/51, score: 67, epsilon: 0.6465587967553006\n",
      "Episode 4/51, score: 30, epsilon: 0.5562889678716474\n",
      "Episode 5/51, score: 93, epsilon: 0.34901730169741024\n",
      "Episode 6/51, score: 44, epsilon: 0.2799384215094006\n",
      "Episode 7/51, score: 30, epsilon: 0.2408545925762412\n",
      "Episode 8/51, score: 32, epsilon: 0.20516038984972615\n",
      "Episode 9/51, score: 17, epsilon: 0.18840216465300522\n",
      "Episode 10/51, score: 30, epsilon: 0.16209824418995536\n",
      "Episode 11/51, score: 62, epsilon: 0.11879805134519765\n",
      "Episode 12/51, score: 121, epsilon: 0.06477420436570952\n",
      "Episode 13/51, score: 44, epsilon: 0.05195383849590569\n",
      "Episode 14/51, score: 41, epsilon: 0.04230229704853423\n",
      "Episode 15/51, score: 98, epsilon: 0.025883670561501974\n",
      "Episode 16/51, score: 141, epsilon: 0.012766746905164949\n",
      "Episode 17/51, score: 146, epsilon: 0.00998645168764533\n",
      "Episode 19/51, score: 119, epsilon: 0.00998645168764533\n",
      "Episode 20/51, score: 95, epsilon: 0.00998645168764533\n",
      "Episode 21/51, score: 88, epsilon: 0.00998645168764533\n",
      "Episode 22/51, score: 136, epsilon: 0.00998645168764533\n",
      "Episode 23/51, score: 155, epsilon: 0.00998645168764533\n",
      "Episode 24/51, score: 142, epsilon: 0.00998645168764533\n",
      "Episode 25/51, score: 188, epsilon: 0.00998645168764533\n",
      "Episode 26/51, score: 129, epsilon: 0.00998645168764533\n",
      "Episode 27/51, score: 140, epsilon: 0.00998645168764533\n",
      "Episode 28/51, score: 163, epsilon: 0.00998645168764533\n",
      "Episode 29/51, score: 154, epsilon: 0.00998645168764533\n",
      "Episode 30/51, score: 163, epsilon: 0.00998645168764533\n",
      "Episode 31/51, score: 149, epsilon: 0.00998645168764533\n",
      "Episode 32/51, score: 173, epsilon: 0.00998645168764533\n",
      "Episode 33/51, score: 8, epsilon: 0.00998645168764533\n",
      "Episode 34/51, score: 147, epsilon: 0.00998645168764533\n",
      "Episode 35/51, score: 77, epsilon: 0.00998645168764533\n",
      "Episode 36/51, score: 80, epsilon: 0.00998645168764533\n",
      "Episode 37/51, score: 75, epsilon: 0.00998645168764533\n",
      "Episode 38/51, score: 69, epsilon: 0.00998645168764533\n",
      "Episode 39/51, score: 130, epsilon: 0.00998645168764533\n",
      "Episode 40/51, score: 149, epsilon: 0.00998645168764533\n",
      "Episode 41/51, score: 189, epsilon: 0.00998645168764533\n",
      "Episode 42/51, score: 62, epsilon: 0.00998645168764533\n",
      "Episode 43/51, score: 66, epsilon: 0.00998645168764533\n",
      "Episode 44/51, score: 118, epsilon: 0.00998645168764533\n",
      "Episode 46/51, score: 160, epsilon: 0.00998645168764533\n",
      "Episode 47/51, score: 9, epsilon: 0.00998645168764533\n",
      "Episode 50/51, score: 76, epsilon: 0.00998645168764533\n"
     ]
    }
   ],
   "source": [
    "# if done = true, then either the pole has dropped off the cart or it manages to stay balanced for 200 time units\n",
    "# which is the maximum\n",
    "done = False\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    \n",
    "    for time in range(200):\n",
    "\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        reward = reward if not done else -10\n",
    "        \n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        \n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            print(\"Episode {}/{}, score: {}, epsilon: {}\".format(episode, n_episodes, time, agent.epsilon))\n",
    "            break\n",
    "        \n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "            \n",
    "        if episode % 50 == 0:\n",
    "            agent.save(output_dir + \"weights_\" + \"{:04d}\".format(episode) + \".hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score is the measure of performance. After only 50 episodes, our agent is already capable of staying balanced for almost the maximum amount of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
